/**
 * Script pour configurer un mod√®le ML local (Ollama)
 * Usage: node scripts/setup-local-ml.js
 */

require('dotenv').config();
const mongoose = require('mongoose');
const AIModel = require('../models/AIModel');
const LocalMLService = require('../services/localMLService');

const MONGODB_URI = process.env.MONGODB_URI || 'mongodb://localhost:27017/reversee';

async function setupLocalML() {
  try {
    // Connect to MongoDB
    await mongoose.connect(MONGODB_URI, {
      useNewUrlParser: true,
      useUnifiedTopology: true,
    });
    console.log('‚úÖ Connected to MongoDB');

    // V√©rifier si Ollama est disponible
    console.log('\nüîç V√©rification de la disponibilit√© d\'Ollama...');
    const availability = await LocalMLService.checkLocalModelAvailability('llama3');
    
    if (!availability.available) {
      console.error('‚ùå Ollama n\'est pas disponible');
      console.log('üí° Installez Ollama depuis https://ollama.ai');
      console.log('üí° D√©marrez Ollama et r√©essayez');
      await mongoose.connection.close();
      process.exit(1);
    }

    console.log('‚úÖ Ollama est disponible');
    
    // V√©rifier si le mod√®le llama3 est install√©
    if (!availability.modelExists) {
      console.log('\n‚ö†Ô∏è  Le mod√®le llama3 n\'est pas install√©.');
      console.log('üí° T√©l√©chargez-le avec l\'une de ces m√©thodes:');
      console.log('   1. Via l\'interface Ollama (recommand√©)');
      console.log('   2. Via la ligne de commande: ollama pull llama3');
      console.log('   3. Via le script: node scripts/download-llama3.js');
      console.log('\nüîÑ Configuration du mod√®le quand m√™me... (il sera t√©l√©charg√© automatiquement au premier appel)');
    } else {
      console.log('‚úÖ Le mod√®le llama3 est d√©j√† install√©');
    }

    // D√©sactiver tous les autres mod√®les par d√©faut
    await AIModel.updateMany(
      { isDefault: true },
      { isDefault: false }
    );

    // V√©rifier si un mod√®le local existe d√©j√†
    let localModel = await AIModel.findOne({ 
      provider: 'local',
      modelId: 'llama3'
    });

    const endpoint = process.env.LOCAL_AI_ENDPOINT || 'http://localhost:11434';

    if (localModel) {
      // Mettre √† jour le mod√®le existant
      localModel.isActive = true;
      localModel.isDefault = true;
      localModel.apiEndpoint = endpoint;
      localModel.config = {
        temperature: 0.7,
        maxTokens: 1000,
        topP: 0.9,
        frequencyPenalty: 0,
        presencePenalty: 0
      };
      localModel.systemPrompt = `Vous √™tes un assistant IA sp√©cialis√© dans le suivi et l'am√©lioration des habitudes de vie. Vous donnez des conseils personnalis√©s, encourageants et pratiques pour aider les utilisateurs √† am√©liorer leur bien-√™tre.`;
      await localModel.save();
      console.log('‚úÖ Mod√®le ML local mis √† jour et d√©fini comme mod√®le par d√©faut');
    } else {
      // Cr√©er un nouveau mod√®le local
      localModel = await AIModel.create({
        name: 'Llama 3 (Local ML)',
        provider: 'local',
        modelId: 'llama3',
        description: 'Mod√®le ML local Llama 3 via Ollama - Gratuit et sans quota',
        config: {
          temperature: 0.7,
          maxTokens: 1000,
          topP: 0.9,
          frequencyPenalty: 0,
          presencePenalty: 0
        },
        systemPrompt: `Vous √™tes un assistant IA sp√©cialis√© dans le suivi et l'am√©lioration des habitudes de vie. Vous donnez des conseils personnalis√©s, encourageants et pratiques pour aider les utilisateurs √† am√©liorer leur bien-√™tre. R√©pondez toujours en fran√ßais, soyez naturel, chaleureux et encourageant.`,
        apiEndpoint: endpoint,
        isActive: true,
        isDefault: true,
        requiresApiKey: false
      });
      console.log('‚úÖ Mod√®le ML local cr√©√© et d√©fini comme mod√®le par d√©faut');
    }

    console.log('\nüìã Configuration du mod√®le:');
    console.log(`   Nom: ${localModel.name}`);
    console.log(`   Provider: ${localModel.provider}`);
    console.log(`   Model ID: ${localModel.modelId}`);
    console.log(`   Endpoint: ${localModel.apiEndpoint}`);
    console.log(`   Temperature: ${localModel.config.temperature}`);
    console.log(`   Max Tokens: ${localModel.config.maxTokens}`);
    console.log(`   Actif: ${localModel.isActive}`);
    console.log(`   Par d√©faut: ${localModel.isDefault}`);

    console.log('\n‚úÖ Configuration du mod√®le ML local termin√©e avec succ√®s!');
    console.log('üí° Toutes les r√©ponses de l\'Assistant IA utiliseront maintenant le mod√®le ML local.');
    console.log('üí∞ Avantage: Gratuit, sans quota, et vos donn√©es restent locales!');

    await mongoose.connection.close();
    console.log('‚úÖ Connexion MongoDB ferm√©e');
  } catch (error) {
    console.error('‚ùå Erreur:', error);
    process.exit(1);
  }
}

// Ex√©cuter le script
setupLocalML();

