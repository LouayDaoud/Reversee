/**
 * Script pour tÃ©lÃ©charger llama3 via l'API Ollama
 * Usage: node scripts/download-llama3.js
 */

require('dotenv').config();
const axios = require('axios');

const endpoint = process.env.LOCAL_AI_ENDPOINT || 'http://localhost:11434';
const modelName = 'llama3';

async function downloadModel() {
  console.log(`ğŸ“¥ TÃ©lÃ©chargement du modÃ¨le ${modelName} via Ollama...\n`);
  
  try {
    console.log('ğŸ”„ DÃ©marrage du tÃ©lÃ©chargement (cela peut prendre plusieurs minutes)...');
    
    const response = await axios.post(
      `${endpoint}/api/pull`,
      {
        name: modelName,
        stream: false
      },
      {
        timeout: 600000, // 10 minutes pour le tÃ©lÃ©chargement
        onDownloadProgress: (progressEvent) => {
          if (progressEvent.total) {
            const percentCompleted = Math.round((progressEvent.loaded * 100) / progressEvent.total);
            process.stdout.write(`\rğŸ“¥ Progression: ${percentCompleted}%`);
          }
        }
      }
    );
    
    console.log('\nâœ… ModÃ¨le tÃ©lÃ©chargÃ© avec succÃ¨s!');
    console.log(`\nğŸ’¡ Vous pouvez maintenant configurer le modÃ¨le avec:`);
    console.log(`   node scripts/setup-local-ml.js`);
    
  } catch (error) {
    if (error.code === 'ECONNREFUSED') {
      console.error('\nâŒ Ollama n\'est pas dÃ©marrÃ©.');
      console.log('ğŸ’¡ DÃ©marrez Ollama et rÃ©essayez.');
    } else if (error.response) {
      console.error('\nâŒ Erreur:', error.response.data);
    } else {
      console.error('\nâŒ Erreur:', error.message);
    }
    process.exit(1);
  }
}

downloadModel();



