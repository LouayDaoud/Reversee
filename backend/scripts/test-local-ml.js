/**
 * Script pour tester le mod√®le ML local (Ollama)
 * Usage: node scripts/test-local-ml.js
 */

require('dotenv').config();
const LocalMLService = require('../services/localMLService');

async function testLocalML() {
  console.log('üöÄ Test du mod√®le ML local (Ollama)...\n');

  // 1. V√©rifier la disponibilit√©
  const modelName = process.argv[2] || 'gemma3:4b'; // Utilise gemma3:4b par d√©faut
  console.log(`1Ô∏è‚É£  V√©rification de la disponibilit√© d'Ollama et du mod√®le ${modelName}...`);
  const availability = await LocalMLService.checkLocalModelAvailability(modelName);
  
  if (!availability.available) {
    console.error('‚ùå Ollama n\'est pas disponible');
    console.log('üí° Installez Ollama depuis https://ollama.ai');
    console.log('üí° D√©marrez Ollama et r√©essayez');
    process.exit(1);
  }

  console.log('‚úÖ Ollama est disponible');
  console.log(`   Mod√®les disponibles: ${availability.models.join(', ')}`);

  if (!availability.modelExists) {
    console.log(`\n‚ö†Ô∏è  Le mod√®le ${modelName} n'est pas encore visible via l'API.`);
    console.log(`üí° Si vous venez de le t√©l√©charger, attendez quelques secondes.`);
    console.log(`üí° Ou t√©l√©chargez-le avec: ollama pull ${modelName}`);
    console.log(`\nüîÑ Test quand m√™me... (le mod√®le sera charg√© lors du premier appel)\n`);
  } else {
    console.log(`‚úÖ Le mod√®le ${modelName} est install√©\n`);
  }

  // 2. Tester une r√©ponse simple
  console.log('2Ô∏è‚É£  Test de g√©n√©ration de r√©ponse...');
  try {
    const testMessage = 'Bonjour ! Peux-tu me dire bonjour en fran√ßais ?';
    console.log(`   Message de test: "${testMessage}"`);
    console.log(`   Mod√®le utilis√©: ${modelName}`);
    
    // Utiliser directement callLocalModel pour √©viter les probl√®mes avec enrichContext
    const messages = [
      {
        role: 'system',
        content: 'Tu es un assistant IA. R√©ponds toujours en fran√ßais de mani√®re naturelle et chaleureuse.'
      },
      {
        role: 'user',
        content: testMessage
      }
    ];
    
    const response = await LocalMLService.callLocalModel(messages, modelName);

    console.log(`‚úÖ R√©ponse re√ßue:`);
    console.log(`   "${response}"\n`);
    console.log('üéâ Test r√©ussi ! Le mod√®le ML local fonctionne correctement.');
  } catch (error) {
    console.error('‚ùå Erreur lors du test:', error.message);
    process.exit(1);
  }
}

testLocalML();

