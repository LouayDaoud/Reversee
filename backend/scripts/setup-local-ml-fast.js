/**
 * Script pour configurer un mod√®le ML local rapide (mistral ou phi)
 * Usage: node scripts/setup-local-ml-fast.js [modelName]
 * Mod√®les recommand√©s: mistral (rapide), phi (tr√®s l√©ger), gemma2:2b (l√©ger)
 */

require('dotenv').config();
const mongoose = require('mongoose');
const AIModel = require('../models/AIModel');
const LocalMLService = require('../services/localMLService');

const MONGODB_URI = process.env.MONGODB_URI || 'mongodb://localhost:27017/reversee';
const MODEL_NAME = process.argv[2] || 'mistral'; // Utilise mistral par d√©faut (plus rapide que llama3)

async function setupLocalML() {
  try {
    // Connect to MongoDB
    await mongoose.connect(MONGODB_URI, {
      useNewUrlParser: true,
      useUnifiedTopology: true,
    });
    console.log('‚úÖ Connect√© √† MongoDB');

    // V√©rifier si Ollama est disponible
    console.log(`\nüîç V√©rification de la disponibilit√© d'Ollama et du mod√®le ${MODEL_NAME}...`);
    const availability = await LocalMLService.checkLocalModelAvailability(MODEL_NAME);
    
    if (!availability.available) {
      console.error('‚ùå Ollama n\'est pas disponible');
      console.log('üí° Assurez-vous qu\'Ollama est d√©marr√©');
      await mongoose.connection.close();
      process.exit(1);
    }

    console.log('‚úÖ Ollama est disponible');
    
    if (availability.models.length > 0) {
      console.log(`üìã Mod√®les disponibles: ${availability.models.join(', ')}`);
    }

    // V√©rifier si le mod√®le est install√©
    if (!availability.modelExists) {
      console.log(`\n‚ö†Ô∏è  Le mod√®le ${MODEL_NAME} n'est pas encore install√©.`);
      console.log(`üí° T√©l√©chargez-le via l'interface Ollama ou avec: ollama pull ${MODEL_NAME}`);
      console.log(`\nüí° Mod√®les rapides recommand√©s:`);
      console.log(`   - mistral (4.1 GB) - Rapide et efficace`);
      console.log(`   - phi (1.6 GB) - Tr√®s l√©ger, tr√®s rapide`);
      console.log(`   - gemma2:2b (1.4 GB) - L√©ger et performant`);
      console.log(`\nüîÑ Configuration du mod√®le quand m√™me... (il sera t√©l√©charg√© au premier appel)`);
    } else {
      console.log(`‚úÖ Le mod√®le ${MODEL_NAME} est install√©`);
    }

    // D√©sactiver tous les autres mod√®les par d√©faut
    await AIModel.updateMany(
      { isDefault: true },
      { isDefault: false }
    );

    // V√©rifier si un mod√®le local existe d√©j√†
    let localModel = await AIModel.findOne({ 
      provider: 'local',
      modelId: MODEL_NAME
    });

    const endpoint = process.env.LOCAL_AI_ENDPOINT || 'http://localhost:11434';

    const modelNames = {
      'mistral': 'Mistral (Local ML - Rapide)',
      'phi': 'Phi (Local ML - Tr√®s L√©ger)',
      'gemma2:2b': 'Gemma 2:2b (Local ML - L√©ger)',
      'llama3': 'Llama 3 (Local ML)'
    };

    if (localModel) {
      // Mettre √† jour le mod√®le existant
      localModel.isActive = true;
      localModel.isDefault = true;
      localModel.apiEndpoint = endpoint;
      localModel.config = {
        temperature: 0.7,
        maxTokens: 1000,
        topP: 0.9,
        frequencyPenalty: 0,
        presencePenalty: 0
      };
      localModel.systemPrompt = `Vous √™tes un assistant IA sp√©cialis√© dans le suivi et l'am√©lioration des habitudes de vie. Vous donnez des conseils personnalis√©s, encourageants et pratiques pour aider les utilisateurs √† am√©liorer leur bien-√™tre. R√©pondez toujours en fran√ßais, soyez naturel, chaleureux et encourageant.`;
      await localModel.save();
      console.log(`‚úÖ Mod√®le ML local (${MODEL_NAME}) mis √† jour et d√©fini comme mod√®le par d√©faut`);
    } else {
      // Cr√©er un nouveau mod√®le local
      localModel = await AIModel.create({
        name: modelNames[MODEL_NAME] || `${MODEL_NAME} (Local ML)`,
        provider: 'local',
        modelId: MODEL_NAME,
        description: `Mod√®le ML local ${MODEL_NAME} via Ollama - Gratuit, sans quota, et rapide`,
        config: {
          temperature: 0.7,
          maxTokens: 1000,
          topP: 0.9,
          frequencyPenalty: 0,
          presencePenalty: 0
        },
        systemPrompt: `Vous √™tes un assistant IA sp√©cialis√© dans le suivi et l'am√©lioration des habitudes de vie. Vous donnez des conseils personnalis√©s, encourageants et pratiques pour aider les utilisateurs √† am√©liorer leur bien-√™tre. R√©pondez toujours en fran√ßais, soyez naturel, chaleureux et encourageant.`,
        apiEndpoint: endpoint,
        isActive: true,
        isDefault: true,
        requiresApiKey: false
      });
      console.log(`‚úÖ Mod√®le ML local (${MODEL_NAME}) cr√©√© et d√©fini comme mod√®le par d√©faut`);
    }

    console.log('\nüìã Configuration du mod√®le:');
    console.log(`   Nom: ${localModel.name}`);
    console.log(`   Provider: ${localModel.provider}`);
    console.log(`   Model ID: ${localModel.modelId}`);
    console.log(`   Endpoint: ${localModel.apiEndpoint}`);
    console.log(`   Temperature: ${localModel.config.temperature}`);
    console.log(`   Max Tokens: ${localModel.config.maxTokens}`);
    console.log(`   Actif: ${localModel.isActive}`);
    console.log(`   Par d√©faut: ${localModel.isDefault}`);

    console.log('\n‚úÖ Configuration termin√©e!');
    console.log('üí° Le mod√®le sera utilis√© automatiquement par le chatbot.');
    console.log('‚ö° Avantage: Plus rapide et plus l√©ger que llama3!');

    await mongoose.connection.close();
    console.log('‚úÖ Connexion MongoDB ferm√©e');
  } catch (error) {
    console.error('‚ùå Erreur:', error);
    process.exit(1);
  }
}

// Ex√©cuter le script
setupLocalML();



