/**
 * Script pour configurer le mod√®le ML local Gemma 3:4b (ou autre mod√®le)
 * Usage: node scripts/setup-local-ml-gemma.js [modelName]
 */

require('dotenv').config();
const mongoose = require('mongoose');
const AIModel = require('../models/AIModel');
const LocalMLService = require('../services/localMLService');

const MONGODB_URI = process.env.MONGODB_URI || 'mongodb://localhost:27017/reversee';
const MODEL_NAME = process.argv[2] || 'gemma3:4b'; // Utilise gemma3:4b par d√©faut

async function setupLocalML() {
  try {
    // Connect to MongoDB
    await mongoose.connect(MONGODB_URI, {
      useNewUrlParser: true,
      useUnifiedTopology: true,
    });
    console.log('‚úÖ Connect√© √† MongoDB');

    // V√©rifier si Ollama est disponible
    console.log(`\nüîç V√©rification de la disponibilit√© d'Ollama et du mod√®le ${MODEL_NAME}...`);
    const availability = await LocalMLService.checkLocalModelAvailability(MODEL_NAME);
    
    if (!availability.available) {
      console.error('‚ùå Ollama n\'est pas disponible');
      console.log('üí° Assurez-vous qu\'Ollama est d√©marr√©');
      await mongoose.connection.close();
      process.exit(1);
    }

    console.log('‚úÖ Ollama est disponible');
    
    if (availability.models.length > 0) {
      console.log(`üìã Mod√®les disponibles: ${availability.models.join(', ')}`);
    }

    // V√©rifier si le mod√®le est install√©
    if (!availability.modelExists) {
      console.log(`\n‚ö†Ô∏è  Le mod√®le ${MODEL_NAME} n'est pas encore visible via l'API.`);
      console.log(`üí° Si vous venez de le t√©l√©charger, attendez quelques secondes et r√©essayez.`);
      console.log(`üí° Ou t√©l√©chargez-le via l'interface Ollama ou avec: ollama pull ${MODEL_NAME}`);
      console.log(`\nüîÑ Tentative de configuration quand m√™me... (le mod√®le sera test√© lors de la premi√®re utilisation)`);
    } else {
      console.log(`‚úÖ Le mod√®le ${MODEL_NAME} est install√©`);
    }

    // D√©sactiver tous les autres mod√®les par d√©faut
    await AIModel.updateMany(
      { isDefault: true },
      { isDefault: false }
    );

    // V√©rifier si un mod√®le local existe d√©j√†
    let localModel = await AIModel.findOne({ 
      provider: 'local',
      modelId: MODEL_NAME
    });

    const endpoint = process.env.LOCAL_AI_ENDPOINT || 'http://localhost:11434';

    if (localModel) {
      // Mettre √† jour le mod√®le existant
      localModel.isActive = true;
      localModel.isDefault = true;
      localModel.apiEndpoint = endpoint;
      localModel.config = {
        temperature: 0.7,
        maxTokens: 1000,
        topP: 0.9,
        frequencyPenalty: 0,
        presencePenalty: 0
      };
      localModel.systemPrompt = `Vous √™tes un assistant IA sp√©cialis√© dans le suivi et l'am√©lioration des habitudes de vie. Vous donnez des conseils personnalis√©s, encourageants et pratiques pour aider les utilisateurs √† am√©liorer leur bien-√™tre. R√©pondez toujours en fran√ßais, soyez naturel, chaleureux et encourageant.`;
      await localModel.save();
      console.log(`‚úÖ Mod√®le ML local (${MODEL_NAME}) mis √† jour et d√©fini comme mod√®le par d√©faut`);
    } else {
      // Cr√©er un nouveau mod√®le local
      localModel = await AIModel.create({
        name: `Gemma 3:4b (Local ML)`,
        provider: 'local',
        modelId: MODEL_NAME,
        description: `Mod√®le ML local ${MODEL_NAME} via Ollama - Gratuit et sans quota`,
        config: {
          temperature: 0.7,
          maxTokens: 1000,
          topP: 0.9,
          frequencyPenalty: 0,
          presencePenalty: 0
        },
        systemPrompt: `Vous √™tes un assistant IA sp√©cialis√© dans le suivi et l'am√©lioration des habitudes de vie. Vous donnez des conseils personnalis√©s, encourageants et pratiques pour aider les utilisateurs √† am√©liorer leur bien-√™tre. R√©pondez toujours en fran√ßais, soyez naturel, chaleureux et encourageant.`,
        apiEndpoint: endpoint,
        isActive: true,
        isDefault: true,
        requiresApiKey: false
      });
      console.log(`‚úÖ Mod√®le ML local (${MODEL_NAME}) cr√©√© et d√©fini comme mod√®le par d√©faut`);
    }

    console.log('\nüìã Configuration du mod√®le:');
    console.log(`   Nom: ${localModel.name}`);
    console.log(`   Provider: ${localModel.provider}`);
    console.log(`   Model ID: ${localModel.modelId}`);
    console.log(`   Endpoint: ${localModel.apiEndpoint}`);
    console.log(`   Temperature: ${localModel.config.temperature}`);
    console.log(`   Max Tokens: ${localModel.config.maxTokens}`);
    console.log(`   Actif: ${localModel.isActive}`);
    console.log(`   Par d√©faut: ${localModel.isDefault}`);

    console.log('\n‚úÖ Configuration du mod√®le ML local termin√©e avec succ√®s!');
    console.log('üí° Toutes les r√©ponses de l\'Assistant IA utiliseront maintenant le mod√®le ML local.');
    console.log('üí∞ Avantage: Gratuit, sans quota, et vos donn√©es restent locales!');

    await mongoose.connection.close();
    console.log('‚úÖ Connexion MongoDB ferm√©e');
  } catch (error) {
    console.error('‚ùå Erreur:', error);
    process.exit(1);
  }
}

// Ex√©cuter le script
setupLocalML();



