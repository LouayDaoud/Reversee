/**
 * Script pour tÃ©lÃ©charger llama3 via l'API Ollama (sans commande CLI)
 * Usage: node scripts/pull-llama3-api.js
 */

require('dotenv').config();
const axios = require('axios');

const endpoint = process.env.LOCAL_AI_ENDPOINT || 'http://localhost:11434';

async function pullModel() {
  console.log('ğŸ“¥ TÃ©lÃ©chargement de llama3 via l\'API Ollama...\n');
  console.log('â³ Cela peut prendre plusieurs minutes selon votre connexion internet...\n');
  
  try {
    const response = await axios.post(
      `${endpoint}/api/pull`,
      {
        name: 'llama3',
        stream: false
      },
      {
        timeout: 600000, // 10 minutes
        onDownloadProgress: (progressEvent) => {
          if (progressEvent.total) {
            const percent = Math.round((progressEvent.loaded * 100) / progressEvent.total);
            process.stdout.write(`\rğŸ“¥ Progression: ${percent}%`);
          } else {
            process.stdout.write(`\rğŸ“¥ TÃ©lÃ©chargement en cours...`);
          }
        }
      }
    );
    
    console.log('\n\nâœ… ModÃ¨le llama3 tÃ©lÃ©chargÃ© avec succÃ¨s!');
    console.log('\nğŸ’¡ Le modÃ¨le est maintenant prÃªt Ã  Ãªtre utilisÃ© dans votre application.');
    console.log('ğŸ’¡ RedÃ©marrez votre backend si nÃ©cessaire et testez le chatbot.');
    
  } catch (error) {
    if (error.code === 'ECONNREFUSED') {
      console.error('\nâŒ Ollama n\'est pas dÃ©marrÃ©.');
      console.log('ğŸ’¡ Assurez-vous que l\'application Ollama est ouverte.');
    } else if (error.response?.status === 404) {
      console.error('\nâŒ Erreur: ModÃ¨le non trouvÃ©');
      console.log('ğŸ’¡ VÃ©rifiez que le nom du modÃ¨le est correct: llama3');
    } else {
      console.error('\nâŒ Erreur:', error.message);
      if (error.response?.data) {
        console.error('   DÃ©tails:', error.response.data);
      }
    }
    process.exit(1);
  }
}

pullModel();



