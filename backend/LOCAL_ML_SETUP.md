# ü§ñ Configuration d'un Mod√®le ML Local pour le Chatbot

Ce guide vous explique comment configurer un mod√®le Machine Learning local pour votre chatbot, utilisant **Ollama** (gratuit et sans quota).

## üìã Avantages d'un Mod√®le ML Local

- ‚úÖ **Gratuit** - Pas de co√ªts d'API
- ‚úÖ **Sans quota** - Utilisation illimit√©e
- ‚úÖ **Donn√©es locales** - Vos conversations restent sur votre machine
- ‚úÖ **Rapide** - Pas de latence r√©seau
- ‚úÖ **Personnalisable** - Vous pouvez entra√Æner votre propre mod√®le

## üöÄ Installation d'Ollama

### Windows

1. T√©l√©chargez Ollama depuis https://ollama.ai/download
2. Installez l'application
3. Ollama d√©marrera automatiquement en arri√®re-plan

### macOS

```bash
brew install ollama
```

### Linux

```bash
curl -fsSL https://ollama.ai/install.sh | sh
```

## üì• T√©l√©charger un Mod√®le

Une fois Ollama install√©, t√©l√©chargez un mod√®le (recommand√©: **llama3**):

```bash
ollama pull llama3
```

Autres mod√®les disponibles:
- `llama3` - Mod√®le g√©n√©raliste, bon √©quilibre (recommand√©)
- `llama3:8b` - Version plus l√©g√®re
- `mistral` - Mod√®le rapide et efficace
- `codellama` - Sp√©cialis√© pour le code
- `phi` - Mod√®le tr√®s l√©ger

## ‚öôÔ∏è Configuration dans l'Application

### √âtape 1: V√©rifier qu'Ollama fonctionne

```bash
ollama list
```

Vous devriez voir la liste des mod√®les t√©l√©charg√©s.

### √âtape 2: Configurer le mod√®le ML local

Ex√©cutez le script de configuration:

```bash
cd backend
node scripts/setup-local-ml.js
```

Ce script va:
- ‚úÖ V√©rifier qu'Ollama est disponible
- ‚úÖ T√©l√©charger llama3 si n√©cessaire
- ‚úÖ Cr√©er/configurer le mod√®le dans la base de donn√©es
- ‚úÖ Le d√©finir comme mod√®le par d√©faut

### √âtape 3: (Optionnel) Configurer l'endpoint personnalis√©

Si Ollama tourne sur un autre port ou une autre machine, ajoutez dans `backend/.env`:

```env
LOCAL_AI_ENDPOINT=http://localhost:11434
```

## üîÑ Basculer entre OpenAI et Mod√®le Local

### Utiliser le Mod√®le Local (par d√©faut apr√®s setup)

Le mod√®le local sera automatiquement utilis√© apr√®s avoir ex√©cut√© `setup-local-ml.js`.

### Revenir √† OpenAI

```bash
cd backend
node scripts/setup-openai-model-gpt4o.js
```

## üß™ Tester le Mod√®le Local

### Test manuel avec Ollama

```bash
ollama run llama3
```

Puis tapez un message pour tester.

### Test via l'application

1. D√©marrez le backend
2. Ouvrez le chatbot dans l'interface
3. Envoyez un message
4. V√©rifiez les logs du backend pour voir "ü§ñ Utilisation du mod√®le ML local"

## üìä Comparaison des Mod√®les

| Mod√®le | Taille | Vitesse | Qualit√© | RAM Requise |
|--------|--------|---------|---------|-------------|
| llama3 | ~4.7GB | ‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê‚≠ê | 8GB+ |
| llama3:8b | ~4.7GB | ‚≠ê‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê | 8GB+ |
| mistral | ~4.1GB | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê‚≠ê | 8GB+ |
| phi | ~1.6GB | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê | 4GB+ |

## üêõ D√©pannage

### Erreur: "Mod√®le ML local non disponible"

**Solution:**
1. V√©rifiez qu'Ollama est d√©marr√©:
   ```bash
   ollama list
   ```
2. Si Ollama n'est pas d√©marr√©, lancez-le manuellement
3. V√©rifiez que le port 11434 est accessible

### Erreur: "Le mod√®le llama3 n'est pas install√©"

**Solution:**
```bash
ollama pull llama3
```

### Le mod√®le est lent

**Solutions:**
1. Utilisez un mod√®le plus l√©ger: `ollama pull mistral`
2. Mettez √† jour le script pour utiliser mistral au lieu de llama3
3. V√©rifiez que vous avez assez de RAM (8GB+ recommand√©)

### Le mod√®le ne r√©pond pas en fran√ßais

Le mod√®le devrait r√©pondre en fran√ßais gr√¢ce au prompt syst√®me. Si ce n'est pas le cas:
1. V√©rifiez les logs du backend
2. Le prompt syst√®me inclut "R√©pondez toujours en fran√ßais"

## üîß Personnalisation

### Changer le mod√®le utilis√©

Modifiez `backend/scripts/setup-local-ml.js` et changez:
```javascript
const modelName = 'mistral'; // Au lieu de 'llama3'
```

Puis r√©ex√©cutez le script.

### Ajuster les param√®tres

Dans `backend/services/localMLService.js`, vous pouvez modifier:
- `temperature`: Cr√©ativit√© (0.0-1.0)
- `top_p`: Diversit√© des r√©ponses
- `maxTokens`: Longueur maximale des r√©ponses

## üìö Ressources

- [Documentation Ollama](https://github.com/ollama/ollama)
- [Liste des mod√®les disponibles](https://ollama.ai/library)
- [Guide de fine-tuning](https://github.com/ollama/ollama/blob/main/docs/finetuning.md)

## üí° Astuces

1. **Premier d√©marrage**: Le premier appel peut √™tre lent (chargement du mod√®le en m√©moire)
2. **Performance**: Gardez Ollama ouvert pour des r√©ponses plus rapides
3. **Multi-mod√®les**: Vous pouvez avoir plusieurs mod√®les install√©s et basculer entre eux
4. **GPU**: Si vous avez une carte graphique NVIDIA, Ollama l'utilisera automatiquement pour acc√©l√©rer les r√©ponses

## ‚úÖ Checklist de Configuration

- [ ] Ollama install√© et d√©marr√©
- [ ] Mod√®le t√©l√©charg√© (`ollama pull llama3`)
- [ ] Script `setup-local-ml.js` ex√©cut√© avec succ√®s
- [ ] Backend red√©marr√©
- [ ] Chatbot test√© et fonctionnel

---

**Note**: Le mod√®le ML local est une excellente alternative √† OpenAI, surtout si vous avez des probl√®mes de quota ou souhaitez garder vos donn√©es locales.

